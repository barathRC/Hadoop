
RAID 0 (striping), RAID 1 (replication) operation

HDFS - Commodity hardware, Open source software. Easy horizontal scale. Close to 4000 node in a cluster.

ClouderaDH distro for linux. HortonWorks Windows support as well. 

Cloudera Manager is an UI. Install packets instead of packages wherever possible.

Data is stored locally in each DataNode and not as a centralized data center.

SPOF NameNode is tackled by multiple NameNodes, Secondary NameNode, Replicating into an NTP server.

NameNode -
Namenode + JobTracker - Master (Ideally 2, in 400 cluster or more it is atleast 4 or 6)
Datanode + Tasktracker - Slave 

HDFS daemons - Storage
NameNode            - Metadata of file, permissions and blocks holding data 
Secondary NameNode  - Checkpoints primary namenode likely in an hour
DataNode            - Does processing along with data read write in disk i/o

MapRed daemons - Processing
JobTracker          - Manages, coordinates, orchestrates
TaskTracker         - Assigns tasks along with data and creates new JVMs as needed

Default block size is 64 to 128MB. Default RF is 3 which provides concurrency in reads.

HDFS is write once read many. Doesnt support editing data. You need to delete and recreate the entire file.

If you have 10TB data. 8TB of storage capacity. 
Available capacity for storage after the consumptions for all the daemons is 20% which is 2TB.
So, you need to have 2TB x 5-6 nodes.

NAMENODE:-
Namenode is memory hungry. Needs to have High RAM. SPOF. High end hardware SSDs. 
NameNode has data in memory and persists in disk as well as fsImage and Editlogs. 
fsImage - Data at a checkpoint of time. EditLogs - All the changes after the fsImage checkpoin.
Secondary NameNode intercepts Primary NN in an hourly fashion.
You lose HDFS if you lose NameNode. SPOF.
NameNode receives heartbeats every 3 seconds from DataNodes. If 10 heartbeats for an DN fails, it is removed from the cluster.
If the DN is still not responding after 10 minutes the blocks of data are replicated.
If NameNode restarts , it combines fsImage and EditLogs and by thsi it handles SPOF. Else if P NN fails, another NN takes this task.
NN HA:-
Client request is sent to both the NNs.
This SPOF is handled by adding a Stand-by NameNode, Quorum Cluster (separate nodes to track the active Namenode and Standby), Zookeeper cluster.
The Active and Stand-by NN have Quorum Jounra Manager which sends the fsImage and editLogs to Journal Nodes with Epoch number. 
In case of an active NameNode failure, JounralNodes track the highest Epoch number registered and keeps that details as the active NameNode's details.
Zookeeper and JournalNodes have odd no. of nodes.
ZK JVMs in the cluster determines the leader by tracking the data from ZKFC installed in the Namenodes that monitors the NamNodes health.
Use NN HA only when needed. Most case its not even required.
NN Federation:-
You can configure the 2 NN with NN Federation. i.e., NN1 with the path as /prod/ and NN2 with /dev/.
You have the felxibility to isolate and maintain these separately for each environment.


DATANODE:-
Better to have 1TB x 8. DataNodes instead of 2TB x 4 DataNodes 
Client directly writes to DataNode . nothing gets written into NameNode.
Checksum time Write is stored near to the block of data. On Read time Checksum this the data is checked against this WriteTime Checksum file. 

MODES:-
Local mode                - Only 1 JVM in a single machine and all the daemons run here. Ideal for dev and test data.
Pseudo - Distributed mode - 1 machine , separate JVMS for each daemon. Dev and to test data and processing. 5 % - 6% subset of data used 
Fully distributed mode    - Separate nodes for each JVMS. Production. Initially to test use 10% - 15% subset of data   

MAPREDUCE:-
Client submits command to the Job Tracker.
Task Trackers ping their alive status as hearbeats to Job Trackers for every 5 secs.
JT looks for data locality to send the MapRed tasks. 
TT has the data reqd. for processing.
JT sends Map and Red tasks to TT. TT the stands up these Map and/or Red tasks.
MR Task report progress/ complete/ error to TT. TT reports to JT. At this time user sees percentage complete for each tasks in the terminal.
If errored, then JT spins other MR tasks.
Once mappers are complete JT signals TT to tell Reducers to run reduce().
Job is considered finished when the last Map(Map only) or last Reduce(MapRed)  is finished(it signals the parent TT which inturn signals the JT).
_SUCCESS flag is dropped in the o/p hdfs directory.
JT signals all TTs to remove the intermedciate data.
Howwever, log files still persists (JT and tT logs).

MapRed Data Flow:

Input-> Split - 
InputFormat class describes input spec for an MR job. Defines how data is split by the InputFormat. 
Default FileInputFormat splits based on split length ( bytes)
It is resp. for validating the i/p spec of the job. 
Splitting up the i/p files into logical InputSplits ech of which will be assigne dto a mapper.
Provide the RecordReader, which actually reads the rocrds in the i/p file and submits them as key/ value pairs to the mapper.

Map - 
Once i/p data has been split into InputSplits and assigned to mappers, the RecordReader submits the i/p as key,value pairs to the mappers.
The mappers map() method performs the data analysis or other user defined work and emits the resulting key, value pairs to forward to the reducers.
Individual mappers cant communicate with each other in any way and hence not aware of each other. They are solely governed by the local NodeManager.
Each mapping task is equivalent and any mapper can process any i/p file.

Shuffle & Sort -
When mapping phase is complete, the emitted ke,value pairs must be exchanged b/w machines to send all values with the same key toa single reducer. 
This is the only reqd. communication step in MR.

Shuffle: The intermediary key space is partitioned by key subsets and assigned to individual map reduce nodes.
Each map task may emit key,valeu pairs to any partition. All values for the same key are always reduced together regardless of its originated mapper.

Sort: Parfitioner sorts the set of intermed. keys merging the map o/p by key and sorting them by key value.
This allows reducer to work on a continuous chunk of values for each key.

Reduce - 
For eahc key in the partition assigned to the Reducer, the Reducer's reduce() method is called once.
The reduce() method receives a key as well as an iterator over all the values associated with the key. 
The values associated with the key are reurned by the iterator in an undefined order.
Reducer combines or aggregates the values together, returning a single output value as one of the final key, value pairs

Output - 
Hadoop's outputFormat class describes the o/p spec for a mapred job.
It is resp. fpr validating the op spec of the job. ( check taht the o/p directory doesnt already exist).
Provide the RecordWriter , which writes out the o/p files of the job.
O/p files are stored in a file sys.

Combiners -
Combviners are intended as intermed. reducers associatesd with one map task.
If a sum combiner is applied to each mapper o/p the sums of the duplicate keys will be computed before sent across the n/w to the reducer.
This reduces the amt of traffic and make the shuffle and sort much faster.
I/P and O/P data types for the combiner and reducer must be identical.
Combiner outputs zero or more key, value pairs and is specified by reduce method which is caleed to perform this operation.
Often the combiner and reducer are identical whic is possible if the operation is commutative and associative, but not always the case.

Counters - 
Counters are side channelt o mapRed process, but any worker can accumulate (associative addition) data to them.
They allow you to provide application statistics or quality control without running additional jobs.
Users can implement their own per job counters.
All worker noes accumulate to counters (safe for multiprocess).
Counters are often used for job stats to be presented on completion. Better than log messages for diagonizing pblms.
Hadoop maintains bult in job counters - Total bytes of data read and write. Total num of records processed.



SPECULATIVE EXECUTION:-
If a single map or red task runs significantly abt 20% slower than its peers by default, Hadoop tries to spin up the same task on another node.
Whichever completes first is "winner". its op is used. Loser is terminated.
Not recommended for reducers. Turn off soec exec for reducers. By default its on.
mapred.reduce.tasks.speculative.exrecution in mapred-site.xml.


RACK AWARENESS:-
To enable RA, we must supply NN machine with a script that takes multiple IPS and returms a RACK ID.
server123: rack1 server555:rack2 (In a mapping file)
s123-re s555-re (this is embedded in the hostnames).  Try to use this approach as you need to set-up hostnames anyways !!
Provide core-site.xml with full path of your script using "topology.script.file.name" param
After this point HDFS becomes rack aware


AUTHENTICATION & AUTHORIZATION:-
Authorization is outofthebox but Authentication is not. So, Kerberos for Authentication. Else you can configure a protected isolated network to 
separately host service like SSH or Bastion to log into the server.
hdfs PERMISSIONS are linix-like "rwrwrwx" "rw-rw-r" "rw----". Read Write and Browse
Kerberos is a tickegting sys. Microsoft's LDAP is based on this. Control is by ACL and Bastion.
HDFS Quota for new users and veteran users each separately. Lock down a hard limit of usable space and num of files and directories.
Because these directly has a impact in the memory of NameNode. For every space and directory a 200 byte is reserved for the memory by the NameNode.
To achieve use dfsadmin -[SetQuota|setSpaceQuota] or fs -count -q <directory>


HADOOP 1:-
Client apps submit jobs to JOB TRACKER.
JT talks to the NameNode for location of the data.
JT locates the TaskTracker nodes with available slots at or near the data.
JT submits the work to the chosen TT nodes.
The TT nodes are monitored. If they dont submit heart beats , they are deemed to have failed and the work is scheduled on a diff TT.
A TT will notify the JT when a task fails. The JT may resubmit the job elsewhere, it may mark specific record as something to be avoided 
and it may even blocklist the TT as unreliable.
When the work is completed, the JT updates its status.
Client apps can poll the jT for info.
Single NameSpace for the entire cluster is managed by a single NameNode.


Weaknesses -
Tightly coupled to MR. MR only framework.
NN not HA.
JT is bottleneck.
No support for multi-tenancy.

HADOOP 2:-
YARN supports multiple data processing models.
All share the common resource mgmt service.
Data does not needs to be shipped.
GDFS Federation supports for multiple NameNodes/NameSpaces and horizontally scalability of name service
Standby NameNode can be configured to automatically take over if the active NN fails.

ADVANTAGES -
Multi-tenancy: other computation frameworks can be run on the same Hadoop cluster simultaneously.
Single job - queue for multiple job submissions and standardised resource management.
Improved availability and prevention of job failure (NN redundancy, isolation of resource mgmt, job mgmt and task mgmt.
HDFS Federation for multiple namespace. Better isolation in a multi-user and multi-tenant env.















