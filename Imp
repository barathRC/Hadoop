
RAID 0 (striping), RAID 1 (replication) operation

HDFS - Commodity hardware, Open source software. Easy horizontal scale. Close to 4000 node in a cluster.

ClouderaDH distro for linux. HortonWorks Windows support as well. 

Cloudera Manager is an UI. Install packets instead of packages wherever possible.

Data is stored locally in each DataNode and not as a centralized data center.

SPOF NameNode is tackled by multiple NameNodes, Secondary NameNode, Replicating into an NTP server.

NameNode -
Namenode + JobTracker - Master (Ideally 2, in 400 cluster or more it is atleast 4 or 6)
Datanode + Tasktracker - Slave 

HDFS daemons - Storage
NameNode            - Metadata of file, permissions and blocks holding data 
Secondary NameNode  - Checkpoints primary namenode likely in an hour
DataNode            - Does processing along with data read write in disk i/o

MapRed daemons - Processing
JobTracker          - Manages, coordinates, orchestrates
TaskTracker         - Assigns tasks along with data and creates new JVMs as needed

Default block size is 64 to 128MB. Default RF is 3 which provides concurrency in reads.

HDFS is write once read many. Doesnt support editing data. You need to delete and recreate the entire file.

If you have 10TB data. 8TB of storage capacity. 
Available capacity for storage after the consumptions for all the daemons is 20% which is 2TB.
So, you need to have 2TB x 5-6 nodes.

NAMENODE:-
Namenode is memory hungry. Needs to have High RAM. SPOF. High end hardware SSDs. 
NameNode has data in memory and persists in disk as well as fsImage and Editlogs. 
fsImage - Data at a checkpoint of time. EditLogs - All the changes after the fsImage checkpoin.
Secondary NameNode intercepts Primary NN in an hourly fashion.
You lose HDFS if you lose NameNode. SPOF.
NameNode receives heartbeats every 3 seconds from DataNodes. If 10 heartbeats for an DN fails, it is removed from the cluster.
If the DN is still not responding after 10 minutes the blocks of data are replicated.
If NameNode restarts , it combines fsImage and EditLogs and by thsi it handles SPOF. Else if P NN fails, another NN takes this task.
NN HA:-
Client request is sent to both the NNs.
This SPOF is handled by adding a Stand-by NameNode, Quorum Cluster (separate nodes to track the active Namenode and Standby), Zookeeper cluster.
The Active and Stand-by NN have Quorum Jounra Manager which sends the fsImage and editLogs to Journal Nodes with Epoch number. 
In case of an active NameNode failure, JounralNodes track the highest Epoch number registered and keeps that details as the active NameNode's details.
Zookeeper and JournalNodes have odd no. of nodes.
ZK JVMs in the cluster determines the leader by tracking the data from ZKFC installed in the Namenodes that monitors the NamNodes health.
Use NN HA only when needed. Most case its not even required.
NN Federation:-
You can configure the 2 NN with NN Federation. i.e., NN1 with the path as /prod/ and NN2 with /dev/.
You have the felxibility to isolate and maintain these separately for each environment.


DATANODE:-
Better to have 1TB x 8. DataNodes instead of 2TB x 4 DataNodes 
Client directly writes to DataNode . nothing gets written into NameNode.
Checksum time Write is stored near to the block of data. On Read time Checksum this the data is checked against this WriteTime Checksum file. 

Local mode                - Only 1 JVM in a single machine and all the daemons run here. Ideal for dev and test data.
Pseudo - Distributed mode - 1 machine , separate JVMS for each daemon. Dev and to test data and processing. 5 % - 6% subset of data used 
Fully distributed mode    - Separate nodes for each JVMS. Production. Initially to test use 10% - 15% subset of data   
