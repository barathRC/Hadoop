
RAID 0 (striping), RAID 1 (replication) operation

HDFS - Commodity hardware, Open source software. Easy horizontal scale. Close to 4000 node in a cluster.

ClouderaDH distro for linux. HortonWorks Windows support as well. 

Cloudera Manager is an UI. Install packets instead of packages wherever possible.

Data is stored locally in each DataNode and not as a centralized data center.

SPOF NameNode is tackled by multiple NameNodes, Secondary NameNode, Replicating into an NTP server.

NameNode -
Namenode + JobTracker - Master (Ideally 2, in 400 cluster or more it is atleast 4 or 6)
Datanode + Tasktracker - Slave 

HDFS daemons - Storage
NameNode            - Metadata of file, permissions and blocks holding data 
Secondary NameNode  - Checkpoints primary namenode likely in an hour
DataNode            - Does processing along with data read write in disk i/o

MapRed daemons - Processing
JobTracker          - Manages, coordinates, orchestrates
TaskTracker         - Assigns tasks along with data and creates new JVMs as needed

Default block size is 64 to 128MB. Default RF is 3 which provides concurrency in reads.

HDFS is write once read many. Doesnt support editing data. You need to delete and recreate the entire file.

Namenode is memory hungry. Needs to have High RAM. SPOF. High end hardware SSDs. 
NameNode has data in memory and persists in disk as well as fsImage and Editlogs. 
fsImage - Data at a checkpoint of time. EditLogs - All the changes after the fsImage checkpoin.
Secondary NameNode intercepts Primary NN in an hourly fashion.
You lose HDFS if you lose NameNode. SPOF.
NameNode receives heartbeats ecery 3 seconds from DataNodes. If 10 heartbeats for an DN fails, it is removed from the cluster.
If the DN is still not responding after 10 minutes the blocks of data are replicated.

Better to have 1TB x 8. DataNodes instead of 2TB x 4 DataNodes 

Local mode                - Only 1 JVM in a single machine and all the daemons run here. Ideal for dev and test data.
Pseudo - Distributed mode - 1 machine , separate JVMS for each daemon. Dev and to test data and processing. 5 % - 6% subset of data used 
Fully distributed mode    - Separate nodes for each JVMS. Production. Initially to test use 10% - 15% subset of data   
